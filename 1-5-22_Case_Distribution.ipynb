{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from corus import load_morphoru_rnc\n",
    "\n",
    "path = 'RNCgoldInUD_Morpho.conll'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = load_morphoru_rnc(path)\n",
    "\n",
    "corpus = dict()\n",
    "for rec in records:\n",
    "    # check for category\n",
    "    for item in rec.attrs:\n",
    "        if '.xhtml' in item:\n",
    "            category = item.replace('==> ','').replace('.xhtml <==','')\n",
    "    corpus.setdefault(category, [])\n",
    "    corpus[category].append(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Natasha tools for Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipymarkup import show_dep_ascii_markup as show_markup\n",
    "from razdel import sentenize, tokenize\n",
    "from navec import Navec\n",
    "from slovnet import Syntax\n",
    "from slovnet import Morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navec and Slovnet Syntax and Morphology loaded.\n"
     ]
    }
   ],
   "source": [
    "navec = Navec.load('navec_news_v1_1B_250K_300d_100q.tar')\n",
    "syntax = Syntax.load('slovnet_syntax_news_v1.tar')\n",
    "syntax.navec(navec)\n",
    "morph = Morph.load('slovnet_morph_news_v1.tar', batch_size=4)\n",
    "morph.navec(navec)\n",
    "print('Navec and Slovnet Syntax and Morphology loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Natasha tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Однако когда мы приблизились к ним и Мила уже протянула руку , чтобы погладить их , они стремительно сорвались с места и исчезли в чаще .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Однако', 'когда', 'мы', 'приблизились', 'к', 'ним', 'и', 'Мила', 'уже', 'протянула', 'руку', ',', 'чтобы', 'погладить', 'их', ',', 'они', 'стремительно', 'сорвались', 'с', 'места', 'и', 'исчезли', 'в', 'чаще', '.']]\n"
     ]
    }
   ],
   "source": [
    "chunk = []\n",
    "for sent in sentenize(text):\n",
    "    tokens = [_.text for _ in tokenize(sent.text)]\n",
    "    chunk.append(tokens)\n",
    "print(chunk)\n",
    "\n",
    "dependencies = next(syntax.map(chunk))\n",
    "morphologies = next(morph.map(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ┌──────► Однако       advmod\n",
      "          │   ┌──► когда        mark\n",
      "          │   │ ┌► мы           nsubj\n",
      "    ┌──►┌─│ ┌─└─└─ приблизились advcl\n",
      "  ┌►│   │ │ │      к            case\n",
      "  │ │   │ │ └────► ним          obl\n",
      "  │ │   │ │     ┌► и            cc\n",
      "  │ │   │ │   ┌►└─ Мила         nsubj\n",
      "  │ │   │ │   │ ┌► уже          advmod\n",
      "┌─│ │ ┌─│ └───└─└─ протянула    \n",
      "│ │ │ │ │     └──► руку         obj\n",
      "│ │ │ │ │     ┌──► ,            punct\n",
      "│ │ │ │ │     │ ┌► чтобы        mark\n",
      "│ │ │ │ │   ┌►└─└─ погладить    advcl\n",
      "│ │ │ │ │   │ └──► их           obj\n",
      "│ │ │ │ └──►│      ,            punct\n",
      "│ │ │ │     │ ┌──► они          nsubj\n",
      "│ │ │ │     │ │ ┌► стремительно advmod\n",
      "│ │ └─│     └─└─└─ сорвались    \n",
      "│ │   │     │   ┌► с            case\n",
      "│ └───│     └──►└─ места        obl\n",
      "│     │         ┌► и            cc\n",
      "│     └──────►┌─└─ исчезли      conj\n",
      "│             │ ┌► в            case\n",
      "│             └►└─ чаще         obl\n",
      "└────────────────► .            punct\n"
     ]
    }
   ],
   "source": [
    "# Convert CoNLL-style format to source, target indices\n",
    "words, deps = [], []\n",
    "for token in dependencies.tokens:\n",
    "    words.append(token.text)\n",
    "    source = int(token.head_id) - 1\n",
    "    target = int(token.id) - 1\n",
    "    if source > 0 and source != target:  # skip root, loops\n",
    "        deps.append([source, target, token.rel])\n",
    "show_markup(words, deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мы\n",
      "none found\n",
      "\n",
      "ним\n",
      "none found\n",
      "\n",
      "руку\n",
      "none found\n",
      "\n",
      "их\n",
      "none found\n",
      "\n",
      "они\n",
      "none found\n",
      "\n",
      "места\n",
      "PREPOSITION: к места\n",
      "PREPOSITION: с места\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### find the heads of different words. if there is a preposition in the head,\n",
    "# then expect it to be cased for that reason\n",
    "\n",
    "sent_len = len(dependencies.tokens)\n",
    "for i in range(sent_len):\n",
    "\n",
    "    pos = morphologies.tokens[i].pos\n",
    "    itok = morphologies.tokens[i]\n",
    "\n",
    "    if 'Case' in itok.feats and pos in ['NOUN','PRON']:\n",
    "        print(itok.text)\n",
    "        \n",
    "        # check to see if this token is the head of some adp\n",
    "        adpfound = False\n",
    "        adpset = set()\n",
    "        for j in range(sent_len):\n",
    "\n",
    "            jtok = morphologies.tokens[j]\n",
    "            headid = int(dependencies.tokens[j].head_id)\n",
    "            jhead = morphologies.tokens[headid - 1]\n",
    "            \n",
    "            if itok == jhead:\n",
    "                # check if adp\n",
    "                if jtok.pos == 'ADP':\n",
    "                    print('PREPOSITION:',jtok.text, itok.text)\n",
    "                    adpfound = True\n",
    "                    adpset.add((i-j,jtok.text))\n",
    "\n",
    "        if not adpfound:\n",
    "            print('none found')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping prepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppgrouper = {\n",
    "    'в' : 'в/во',\n",
    "    'во' : 'в/во',\n",
    "    'с' : 'с/со',\n",
    "    'со' : 'с/со',\n",
    "    'к' : 'к/ко',\n",
    "    'ко' : 'к/ко',\n",
    "    'без' : 'без/безо',\n",
    "    'безо' : 'без/безо',\n",
    "    'о' : 'о/об/обо',\n",
    "    'об' : 'о/об/обо',\n",
    "    'обо' : 'о/об/обо',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'в/во'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppgrouper['во']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing blogs\n",
      "parsing fiction\n",
      "parsing public\n",
      "parsing science\n",
      "parsing speech\n",
      "Parsed all 98891 records in abridged RNC.\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "\n",
    "lemmacases = dict()\n",
    "\n",
    "cases = ['Nom','Acc','Gen','Loc','Dat','Ins']\n",
    "    \n",
    "for cat in corpus:\n",
    "    print('parsing',cat)\n",
    "    for rec in corpus[cat]:\n",
    "        # get sentence\n",
    "        sent = rec.tokens\n",
    "        sent_text = ' '.join([t.text for t in rec.tokens if t.text != None])\n",
    "\n",
    "        # parse sentence\n",
    "        chunk = []\n",
    "        for sent in sentenize(sent_text):\n",
    "            tokens = [_.text for _ in tokenize(sent.text)]\n",
    "            chunk.append(tokens)\n",
    "        if chunk == [[]]:\n",
    "            continue\n",
    "        # generate dependencies and morphology\n",
    "        dependencies = next(syntax.map(chunk))\n",
    "        morphologies = next(morph.map(chunk))\n",
    "        \n",
    "        # loop over sent tokens\n",
    "        for i, morph_token in enumerate(morphologies.tokens):\n",
    "            bad_morph_token = False\n",
    "            \n",
    "            if 'Case' in morph_token.feats and morph_token.pos in ['PRON','NOUN'] and 'Number' in morph_token.feats:\n",
    "                # get lemma accurately from rnc source\n",
    "                for rec_token in rec.tokens:\n",
    "                    if rec_token.text == morph_token.text:\n",
    "                        lemma = rec_token.lemma\n",
    "                        if 'Case' in rec_token.feats:\n",
    "                            thiscase = rec_token.feats['Case'].upper()\n",
    "                        else:\n",
    "                            bad_morph_token = True\n",
    "                        break\n",
    "                if bad_morph_token:\n",
    "                    continue\n",
    "                    \n",
    "                # set up dictionary\n",
    "#                 thiscase = morph_token.feats['Case']\n",
    "#                 if thiscase in ['Par','Voc']:\n",
    "#                     continue\n",
    "            \n",
    "                lemmacases.setdefault(lemma,dict())\n",
    "                for case in cases:\n",
    "                    uppercase = case.upper()\n",
    "                    lemmacases[lemma].setdefault(uppercase,dict())\n",
    "                    for number in ['Sing','Plur']:\n",
    "                        lemmacases[lemma][uppercase].setdefault(number,dict())\n",
    "                        # set up noprep\n",
    "                        lemmacases[lemma][uppercase][number].setdefault('NO_PREPOSITION',dict())\n",
    "                        lemmacases[lemma][uppercase][number]['NO_PREPOSITION'].setdefault('count',0)\n",
    "                        lemmacases[lemma][uppercase][number]['NO_PREPOSITION'].setdefault('examples',[])\n",
    "                        lemmacases[lemma][uppercase][number]['NO_PREPOSITION'].setdefault('form','')\n",
    "                # search for ADPositions\n",
    "                itok = morphologies.tokens[i]\n",
    "\n",
    "                # check to see if this token is the head of some adp\n",
    "                adpfound = False\n",
    "                adpset = set()\n",
    "                for j in range(len(morphologies.tokens)):\n",
    "\n",
    "                    jtok = morphologies.tokens[j]\n",
    "                    headid = int(dependencies.tokens[j].head_id)\n",
    "                    if headid == 0:\n",
    "                        jhead = 'ROOT'\n",
    "                    else:\n",
    "                        jhead = morphologies.tokens[headid - 1]\n",
    "\n",
    "                    if itok == jhead:\n",
    "                        # check if adp\n",
    "                        if jtok.pos == 'ADP':\n",
    "                            prepphrase = f'{jtok.text.lower()} + {thiscase.upper()}'\n",
    "                            adpfound = True\n",
    "                            adpset.add((i-j, jtok.text))\n",
    "\n",
    "                if not adpfound:\n",
    "                    prepphrase = 'NO_PREPOSITION'\n",
    "                else:\n",
    "                    mindist, prepphrase = list(adpset)[0]\n",
    "                    for distance, adptoken in adpset:\n",
    "                        if distance < mindist:\n",
    "                            mindist = distance\n",
    "                            prepphrase = adptoken\n",
    "                    prepphrase = prepphrase.lower()\n",
    "                    if prepphrase in ppgrouper:\n",
    "                        prepphrase = ppgrouper[prepphrase]\n",
    "                        \n",
    "                qty = morph_token.feats['Number']\n",
    "                \n",
    "                lemmacases[lemma][thiscase][qty].setdefault(prepphrase, dict())\n",
    "                lemmacases[lemma][thiscase][qty][prepphrase].setdefault('examples',[])\n",
    "                lemmacases[lemma][thiscase][qty][prepphrase].setdefault('count',0)\n",
    "\n",
    "                lemmacases[lemma][thiscase][qty][prepphrase]['form'] = itok.text\n",
    "                lemmacases[lemma][thiscase][qty][prepphrase]['examples'].append(sent_text)\n",
    "                lemmacases[lemma][thiscase][qty][prepphrase]['count'] += 1\n",
    "                \n",
    "        ct += 1\n",
    "        \n",
    "print(f'Parsed all {ct} records in abridged RNC.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOM', 'ACC', 'GEN', 'LOC', 'DAT', 'INS']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uppercases = [case.upper() for case in cases]\n",
    "uppercases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOM\n",
      "NO_PREPOSITION Sing облако 1\n",
      "\n",
      "NO_PREPOSITION Plur облака 7\n",
      "\n",
      "\n",
      "ACC\n",
      "NO_PREPOSITION Sing  0\n",
      "\n",
      "NO_PREPOSITION Plur облака 2\n",
      "\n",
      "\n",
      "GEN\n",
      "NO_PREPOSITION Sing облака 1\n",
      "из Sing облака 1\n",
      "\n",
      "NO_PREPOSITION Plur облаков 7\n",
      "из Plur облаков 1\n",
      "до Plur облаков 1\n",
      "\n",
      "\n",
      "LOC\n",
      "NO_PREPOSITION Sing  0\n",
      "на Sing облаке 1\n",
      "в/во Sing облацех 1\n",
      "\n",
      "NO_PREPOSITION Plur  0\n",
      "в/во Plur облаках 2\n",
      "\n",
      "\n",
      "DAT\n",
      "NO_PREPOSITION Sing  0\n",
      "\n",
      "NO_PREPOSITION Plur  0\n",
      "\n",
      "\n",
      "INS\n",
      "NO_PREPOSITION Sing  0\n",
      "\n",
      "NO_PREPOSITION Plur облаками 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keyword = 'облако'\n",
    "\n",
    "for case in uppercases:\n",
    "    print(case)\n",
    "    for number in ['Sing','Plur']:\n",
    "        for pp in lemmacases[keyword][case][number]:\n",
    "            print(pp, number, lemmacases[keyword][case][number][pp]['form'], lemmacases[keyword][case][number][pp]['count'])\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to JSON form for radar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find case/pps which do not seem relevant to include\n",
    "numcount = dict()\n",
    "for lemma in lemmacases:\n",
    "    numcount[lemma] = dict()\n",
    "    for case in ['ACC','NOM','INS','DAT','LOC','GEN']:\n",
    "        numcount[lemma][case] = dict()\n",
    "        for pp in set(list(lemmacases[lemma][case]['Sing'].keys())+list(lemmacases[lemma][case]['Plur'].keys())):\n",
    "            numcount[lemma][case][pp] = 0\n",
    "            for number in ['Sing','Plur']:\n",
    "                if pp in lemmacases[lemma][case][number]:\n",
    "                    numcount[lemma][case][pp] += lemmacases[lemma][case][number][pp]['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful creation of JSON list.\n"
     ]
    }
   ],
   "source": [
    "jsonlist = []\n",
    "\n",
    "totals = dict()\n",
    "\n",
    "for lemma in lemmacases:\n",
    "    thisdict = dict()\n",
    "    thisdict['label'] = lemma\n",
    "    \n",
    "    thisdict['data'] = {}\n",
    "    \n",
    "    totals[lemma] = dict()\n",
    "    \n",
    "    basic = dict()\n",
    "    detailed = dict()\n",
    "    \n",
    "    examplecount = 0\n",
    "    \n",
    "    for number in ['Sing','Plur']:\n",
    "        totals[lemma][number] = dict()\n",
    "        \n",
    "        basic[number] = dict()\n",
    "#         basic[number]['numlabel'] = lemmacases[lemma]['Nom'][number]['NO_PREPOSITION']['form']\n",
    "        basic[number]['data'] = []\n",
    "        \n",
    "        detailed[number] = dict()\n",
    "#         detailed[number]['numlabel'] = lemmacases[lemma]['Nom'][number]['NO_PREPOSITION']['form']\n",
    "        detailed[number]['data'] = []\n",
    "        \n",
    "        for case in ['ACC','NOM','INS','DAT','LOC','GEN']:\n",
    "            \n",
    "            # get totals for simple chart\n",
    "            totals[lemma][number][case] = dict()\n",
    "            totals[lemma][number][case]['count'] = 0\n",
    "            totals[lemma][number][case]['form'] = ''\n",
    "            # loop over pps for totals\n",
    "            for pp in lemmacases[lemma][case][number]:\n",
    "                if numcount[lemma][case][pp] < 3:\n",
    "                    continue\n",
    "                totals[lemma][number][case]['count'] += lemmacases[lemma][case][number][pp]['count']\n",
    "                totals[lemma][number][case]['form'] = lemmacases[lemma][case][number][pp]['form']\n",
    "            # populate basic dict\n",
    "            form = totals[lemma][number][case]['form']\n",
    "            if form == 'ё-таки':\n",
    "                continue\n",
    "                \n",
    "            basic[number]['data'].append({\n",
    "                'axis' : case.upper(),\n",
    "                'value' : totals[lemma][number][case]['count'],\n",
    "                'form' : totals[lemma][number][case]['form'].lower()\n",
    "            })\n",
    "            \n",
    "            # detailed dict\n",
    "            for pp in set(list(lemmacases[lemma][case]['Sing'].keys())+list(lemmacases[lemma][case]['Plur'].keys())):\n",
    "                # to REMOVE ANY PREPOSITIONS THAT HAVE LESS THAN 3 ENTRIES!\n",
    "                if numcount[lemma][case][pp] < 3:\n",
    "                    continue\n",
    "                    \n",
    "                if pp not in lemmacases[lemma][case][number]:\n",
    "                    count = 0\n",
    "                    form = ''\n",
    "                else:\n",
    "                    count = lemmacases[lemma][case][number][pp]['count']\n",
    "                    form = lemmacases[lemma][case][number][pp]['form'].lower()\n",
    "                    \n",
    "                    # add to count\n",
    "                    examplecount += lemmacases[lemma][case][number][pp]['count']\n",
    "                \n",
    "                axis = pp\n",
    "                    \n",
    "                if pp == 'ё-таки':\n",
    "                    continue\n",
    "                if pp == 'NO_PREPOSITION':\n",
    "                    axis = case.upper()\n",
    "                else:\n",
    "                    axis = f'{axis} + {case}'\n",
    "                \n",
    "                \n",
    "                detailed[number]['data'].append({\n",
    "                    'axis' : axis,\n",
    "                    'value' : count,\n",
    "                    'form' : form\n",
    "                })\n",
    "            \n",
    "    if examplecount < 10:\n",
    "        continue\n",
    "    thisdict['data']['basic'] = basic\n",
    "    thisdict['data']['detailed'] = detailed\n",
    "    \n",
    "    jsonlist.append(thisdict)\n",
    "print('Successful creation of JSON list.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'подружка',\n",
       " 'data': {'basic': {'Sing': {'data': [{'axis': 'ACC', 'value': 0, 'form': ''},\n",
       "     {'axis': 'NOM', 'value': 7, 'form': 'подружка'},\n",
       "     {'axis': 'INS', 'value': 3, 'form': 'подружкой'},\n",
       "     {'axis': 'DAT', 'value': 0, 'form': ''},\n",
       "     {'axis': 'LOC', 'value': 0, 'form': ''},\n",
       "     {'axis': 'GEN', 'value': 1, 'form': 'подружки'}]},\n",
       "   'Plur': {'data': [{'axis': 'ACC', 'value': 0, 'form': ''},\n",
       "     {'axis': 'NOM', 'value': 4, 'form': 'подружки'},\n",
       "     {'axis': 'INS', 'value': 1, 'form': 'подружками'},\n",
       "     {'axis': 'DAT', 'value': 0, 'form': ''},\n",
       "     {'axis': 'LOC', 'value': 0, 'form': ''},\n",
       "     {'axis': 'GEN', 'value': 3, 'form': 'подружек'}]}},\n",
       "  'detailed': {'Sing': {'data': [{'axis': 'NOM',\n",
       "      'value': 7,\n",
       "      'form': 'подружка'},\n",
       "     {'axis': 'с/со + INS', 'value': 3, 'form': 'подружкой'},\n",
       "     {'axis': 'GEN', 'value': 1, 'form': 'подружки'}]},\n",
       "   'Plur': {'data': [{'axis': 'NOM', 'value': 4, 'form': 'подружки'},\n",
       "     {'axis': 'с/со + INS', 'value': 1, 'form': 'подружками'},\n",
       "     {'axis': 'GEN', 'value': 3, 'form': 'подружек'}]}}}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example\n",
    "random.choice(jsonlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write JSON to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caseradarcharts/1-11-22_case-radar-data.json written successfully.\n"
     ]
    }
   ],
   "source": [
    "# make json file\n",
    "import json \n",
    "\n",
    "filepath = 'caseradarcharts/1-11-22_case-radar-data.json'\n",
    "\n",
    "with open(filepath, 'w', encoding='utf8') as json_file:\n",
    "    json.dump(jsonlist, json_file, ensure_ascii=False)\n",
    "    \n",
    "print(filepath,'written successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate JSON with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemma -> case -> number -> examples\n",
    "# lemmacases[lemma][case][number][pp]['form']\n",
    "examples = dict()\n",
    "\n",
    "totalct = 0\n",
    "for jsonrow in jsonlist:\n",
    "    lemma = jsonrow['label']\n",
    "# for lemma in lemmacases:\n",
    "    examples.setdefault(lemma, dict())\n",
    "    for case in uppercases:\n",
    "        examples[lemma][case] = dict()\n",
    "#         examples[lemma][case]['forms'] = set()\n",
    "#         examples[lemma][case]['examples'] = dict()\n",
    "        for pp in set(list(lemmacases[lemma][case]['Sing'].keys())+list(lemmacases[lemma][case]['Plur'].keys())):\n",
    "            if numcount[lemma][case][pp] < 3:\n",
    "                continue\n",
    "            if pp not in lemmacases[lemma][case]['Sing']:\n",
    "                singlabeled = []\n",
    "            else:\n",
    "                singexamples = lemmacases[lemma][case]['Sing'][pp]['examples']\n",
    "                singlabeled = [ex + ' [Sing]' for ex in singexamples] \n",
    "            if pp not in lemmacases[lemma][case]['Plur']:\n",
    "                plurlabeled = []\n",
    "            else:\n",
    "                plurexamples = lemmacases[lemma][case]['Plur'][pp]['examples']\n",
    "                plurlabeled = [ex + ' [Plur]' for ex in plurexamples]\n",
    "            random.shuffle(singlabeled)\n",
    "            random.shuffle(plurlabeled)\n",
    "            examples[lemma][case][pp] = singlabeled + plurlabeled\n",
    "#             examples[lemma][case]['forms'].add(lemmacase[lemma][case]['Sing'][pp]['form'])\n",
    "#             examples[lemma][case]['forms'].add(lemmacase[lemma][case]['Plur'][pp]['form'])\n",
    "\n",
    "            totalct += len(singlabeled) + len(plurlabeled)\n",
    "\n",
    "#                 examples[lemma][case][pp] = random.shuffle(examples[lemma][case][pp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265964"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder with sentences of each\n",
    "for lemma in examples:\n",
    "    filepath = 'caseradarcharts/1-11_examples2/'+lemma+'.json'\n",
    "\n",
    "    with open(filepath, 'w', encoding='utf8') as json_file:\n",
    "        json.dump(examples[lemma], json_file, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
